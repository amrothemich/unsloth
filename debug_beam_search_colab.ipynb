{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Search Debugging for Unsloth\n",
    "\n",
    "This notebook helps debug the beam search `_reorder_cache` issue in Google Colab with GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the fork with our fixes\n",
    "!git clone https://github.com/amrothemich/unsloth.git\n",
    "!cd unsloth && git checkout fix-reorder-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install unsloth from our local fork\n",
    "!pip install -e unsloth/\n",
    "!pip install transformers datasets accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test beam search with a minimal example\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Load a small model for testing\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/tinyllama-bnb-4bit\",\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Get PEFT model\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_alpha=16,\n",
    ")\n",
    "\n",
    "# Test beam search\n",
    "inputs = tokenizer(\"Hello, how are\", return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=20,\n",
    "        num_beams=2,\n",
    "        num_return_sequences=2,\n",
    "    )\n",
    "    \n",
    "print(\"Beam search successful!\")\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"Sequence {i}: {tokenizer.decode(output, skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it fails, add debug code\n",
    "# Check what module the model thinks it is\n",
    "print(f\"Model class: {model.__class__}\")\n",
    "print(f\"Model module: {model.__class__.__module__}\")\n",
    "print(f\"Has _reorder_cache: {hasattr(model, '_reorder_cache')}\")\n",
    "if hasattr(model, 'base_model'):\n",
    "    print(f\"Base model class: {model.base_model.__class__}\")\n",
    "    print(f\"Base model has _reorder_cache: {hasattr(model.base_model, '_reorder_cache')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}